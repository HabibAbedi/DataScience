{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist \n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout \n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D \n",
        "from tensorflow.keras.layers import LeakyReLU \n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D \n",
        "from tensorflow.keras.models import Sequential, Model \n",
        "from tensorflow.keras.optimizers import Adam \n",
        "from tensorflow.keras import initializers \n",
        "import matplotlib.pyplot as plt \n",
        "import sys \n",
        "import numpy as np \n",
        "import tqdm"
      ],
      "metadata": {
        "id": "l1tIAWQGBwlE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomDim = 10"
      ],
      "metadata": {
        "id": "LZYEfbpUB6TK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST data \n",
        "(X_train, _), (_, _) = mnist.load_data() \n",
        "X_train = (X_train.astype(np.float32) - 127.5)/127.5 \n",
        "X_train = X_train.reshape(60000, 784)"
      ],
      "metadata": {
        "id": "yZDpVHNhDGVq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if os.path.isdir('./images') is False:\n",
        "  os.makedirs('./images')"
      ],
      "metadata": {
        "id": "CWaaQN2rJbje"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Sequential() \n",
        "\n",
        "generator.add(Dense(256, input_dim=randomDim)) \n",
        "              #,kernel_initializer=initializers.RandomNormal(stddev=0.02))) \n",
        "generator.add(LeakyReLU(0.2)) \n",
        "\n",
        "generator.add(Dense(512)) \n",
        "generator.add(LeakyReLU(0.2)) \n",
        "\n",
        "generator.add(Dense(1024)) \n",
        "generator.add(LeakyReLU(0.2)) \n",
        "\n",
        "generator.add(Dense(784, activation='tanh'))"
      ],
      "metadata": {
        "id": "oHzznQxHEBfh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Sequential()\n",
        "\n",
        "discriminator.add(Dense(1024, input_dim=784, \n",
        "                        kernel_initializer=initializers.RandomNormal(stddev=0.02))) \n",
        "discriminator.add(LeakyReLU(0.2)) \n",
        "discriminator.add(Dropout(0.3)) \n",
        "\n",
        "discriminator.add(Dense(512)) \n",
        "discriminator.add(LeakyReLU(0.2)) \n",
        "discriminator.add(Dropout(0.3)) \n",
        "\n",
        "discriminator.add(Dense(256)) \n",
        "discriminator.add(LeakyReLU(0.2)) \n",
        "discriminator.add(Dropout(0.3)) \n",
        "\n",
        "discriminator.add(Dense(1, activation='sigmoid')) "
      ],
      "metadata": {
        "id": "_lW38bamEXBW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "metadata": {
        "id": "vJyA43sQEmAQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined network \n",
        "discriminator.trainable = False \n",
        "ganInput = Input(shape=(randomDim,)) \n",
        "x = generator(ganInput) \n",
        "ganOutput = discriminator(x) \n",
        "gan = Model(inputs=ganInput, outputs=ganOutput) \n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "metadata": {
        "id": "rPfvLOW3ErdP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dLosses = [] \n",
        "gLosses = []"
      ],
      "metadata": {
        "id": "GrMTu0qyE15R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss from each batch \n",
        "def plotLoss(epoch): \n",
        "  plt.figure(figsize=(10, 8)) \n",
        "  plt.plot(dLosses, label='Discriminitive loss') \n",
        "  plt.plot(gLosses, label='Generative loss') \n",
        "  plt.xlabel('Epoch') \n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend() \n",
        "  plt.savefig('images/gan_loss_epoch_%d.png' % epoch)"
      ],
      "metadata": {
        "id": "Ag8jeG9sE4ax"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a wall of generated MNIST images \n",
        "def saveGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)): \n",
        "  noise = np.random.normal(0, 1, size=[examples, randomDim]) \n",
        "  generatedImages = generator.predict(noise) \n",
        "  generatedImages = generatedImages.reshape(examples, 28, 28) \n",
        "  plt.figure(figsize=figsize) \n",
        "  \n",
        "  for i in range(generatedImages.shape[0]): \n",
        "    plt.subplot(dim[0], dim[1], i+1) \n",
        "    plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r') \n",
        "    plt.axis('off') \n",
        "    plt.tight_layout() \n",
        "    plt.savefig('images/gan_generated_image_epoch_%d.png' % epoch)"
      ],
      "metadata": {
        "id": "fXkPsdQHFBZ6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=1, batchSize=128): \n",
        "  batchCount = int(X_train.shape[0] / batchSize) \n",
        "  print ('Epochs:', epochs) \n",
        "  print ('Batch size:', batchSize)\n",
        "  print ('Batches per epoch:', batchCount) \n",
        "  \n",
        "  for e in range(1, epochs+1): \n",
        "    print ('-'*15, 'Epoch %d' % e, '-'*15) \n",
        "    for _ in range(batchCount): \n",
        "      # Get a random set of input noise and images \n",
        "      noise = np.random.normal(0, 1, size=[batchSize, randomDim]) \n",
        "      imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)] \n",
        "      # Generate fake MNIST images \n",
        "      generatedImages = generator.predict(noise) \n",
        "      # print np.shape(imageBatch), np.shape(generatedImages) \n",
        "      X = np.concatenate([imageBatch, generatedImages]) \n",
        "      # Labels for generated and real data \n",
        "      yDis = np.zeros(2*batchSize) \n",
        "      # One-sided label smoothing \n",
        "      yDis[:batchSize] = 0.9\n",
        "      # Train discriminator \n",
        "      discriminator.trainable = True \n",
        "      dloss = discriminator.train_on_batch(X, yDis) \n",
        "      # Train generator \n",
        "      noise = np.random.normal(0, 1, size=[batchSize, randomDim]) \n",
        "      yGen = np.ones(batchSize) \n",
        "      discriminator.trainable = False \n",
        "      gloss = gan.train_on_batch(noise, yGen) \n",
        "      # Store loss of most recent batch from this epoch \n",
        "      dLosses.append(dloss) \n",
        "      gLosses.append(gloss) \n",
        "      if e == 1 or e % 20 == 0: \n",
        "        saveGeneratedImages(e)\n",
        "      # Plot losses from every epoch \n",
        "      plotLoss(e)"
      ],
      "metadata": {
        "id": "N8jyKT3NFMhV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(200, 128)"
      ],
      "metadata": {
        "id": "FBRAzT79FSUi",
        "outputId": "0d42ba0a-5666-4d20-fb26-cd555a6d0884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 200\n",
            "Batch size: 128\n",
            "Batches per epoch: 468\n",
            "--------------- Epoch 1 ---------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Welcome To Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}